


import pandas as pd



# Load the .xlsx file
xlsx_path = "Assgt_clusBathSoap_Data.xlsx"
xls = pd.ExcelFile(xlsx_path)

# Display available sheet names
xls.sheet_names





# Load the main data sheet
df = pd.read_excel(xlsx_path, sheet_name='DM_Sheet')

# Show the first few rows and basic structure
df.head()





# Summary statistics and data types
summary = df.describe(include='all')
data_info = df.info()

# Check for missing values
missing_values = df.isnull().sum()

# Select only numeric columns for plotting
numeric_cols = df.select_dtypes(include='number').columns

# Return summary, missing values, and numeric column info
summary, missing_values[numeric_cols]











import matplotlib.pyplot as plt
import seaborn as sns

# Set visual style
sns.set(style="whitegrid")

# Create individual plots
plt.figure(figsize=(10, 6))
sns.histplot(df['maxBrCd'], bins=20, kde=True)
plt.title('Distribution of Brand Loyalty (maxBrCd)')
plt.xlabel('Max Brand Share')
plt.ylabel('Count')
plt.tight_layout()
# Save figure
plt.savefig("Distribution of Brand Loyalty.png", dpi=300, bbox_inches='tight')

plt.show()


plt.figure(figsize=(10, 6))
sns.scatterplot(x='No. of  Trans', y='Total Volume', data=df, hue='SEC', palette='viridis')
plt.title('Transactions vs. Volume by Socioeconomic Class')
plt.xlabel('Number of Transactions')
plt.ylabel('Total Volume Purchased')
plt.tight_layout()
# Save figure
plt.savefig("Transactions vs. Volume by Socioeconomic Class.png", dpi=300, bbox_inches='tight')

plt.show()


plt.figure(figsize=(10, 6))
sns.boxplot(x='SEX', y='No. of Brands', data=df)
plt.title('Brand Variety by Gender')
plt.xlabel('Gender (1=Male, 2=Female)')
plt.ylabel('Number of Brands Purchased')
plt.tight_layout()
# Save figure
plt.savefig("Brand Variety by Gender.png", dpi=300, bbox_inches='tight')

plt.show()


plt.figure(figsize=(10, 6))
sns.scatterplot(x='Affluence Index', y='No. of Brands', data=df)
plt.title('Affluence vs. Brand Variety')
plt.xlabel('Affluence Index')
plt.ylabel('Number of Brands Purchased')
plt.tight_layout()
# Save figure
plt.savefig("Affluence vs. Brand Variety.png", dpi=300, bbox_inches='tight')

plt.show()









# Calculate brand switching frequency (higher = more switching)
df['Switching_Freq'] = df['Brand Runs'] / df['No. of Brands']

# Promo Sensitivity: Volume Bought Without Promo
plt.figure(figsize=(10, 6))
sns.histplot(df['Pur Vol No Promo - %'], bins=20, kde=True)
plt.title('Promo Sensitivity: Volume Bought Without Promo')
plt.xlabel('Percentage of Purchase Volume (No Promo)')
plt.ylabel('Count')
plt.tight_layout()
# Save figure
plt.savefig("Promo Sensitivity Volume Bought Without Promo.png", dpi=300, bbox_inches='tight')

plt.show()

# Promo Sensitivity: Volume Bought with Promo Type 6
plt.figure(figsize=(10, 6))
sns.histplot(df['Pur Vol Promo 6 %'], bins=20, kde=True)
plt.title('Promo Sensitivity: Volume Bought with Promo Type 6')
plt.xlabel('Percentage of Purchase Volume (Promo 6)')
plt.ylabel('Count')
plt.tight_layout()
# Save figure
plt.savefig("Promo Sensitivity Volume Bought with Promo Type 6.png", dpi=300, bbox_inches='tight')

plt.show()

# Promo Sensitivity: Volume Bought with Other Promos
plt.figure(figsize=(10, 6))
sns.histplot(df['Pur Vol Other Promo %'], bins=20, kde=True)
plt.title('Promo Sensitivity: Volume Bought with Other Promos')
plt.xlabel('Percentage of Purchase Volume (Other Promos)')
plt.ylabel('Count')
plt.tight_layout()
# Save figure
plt.savefig("Promo Sensitivity Volume Bought with Other Promos.png", dpi=300, bbox_inches='tight')

plt.show()

# Brand Switching Frequency (Brand Runs / No. of Brands)
plt.figure(figsize=(10, 6))
sns.histplot(df['Switching_Freq'], bins=20, kde=True)
plt.title('Brand Switching Frequency (Brand Runs / No. of Brands)')
plt.xlabel('Switching Frequency')
plt.ylabel('Count')
plt.tight_layout()
# Save figure
plt.savefig("Brand Switching Frequency.png", dpi=300, bbox_inches='tight')

plt.show()






# Fix column name with trailing space
df.rename(columns={'Avg. Price ': 'Avg. Price'}, inplace=True)

# Select relevant behavioral features for correlation analysis
behavior_cols = [
    'No. of Brands', 'Brand Runs', 'Switching_Freq', 'Total Volume',
    'No. of  Trans', 'Value', 'Trans / Brand Runs', 'Vol/Tran',
    'Avg. Price', 'Pur Vol No Promo - %', 'Pur Vol Promo 6 %',
    'Pur Vol Other Promo %', 'maxBrCd'
]

# Compute the correlation matrix
corr_matrix = df[behavior_cols].corr()

# Plot heatmap
plt.figure(figsize=(14, 10))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm', center=0)
plt.title('Correlation Matrix of Customer Behavior Features')
plt.tight_layout()
# Save figure
plt.savefig("Correlation Matrix of Customer Behavior Features.png", dpi=300, bbox_inches='tight')

plt.show()






# Plot 1: Affluence vs Switching
plt.figure(figsize=(10, 6))
sns.scatterplot(x='Affluence Index', y='Switching_Freq', data=df)
plt.title('Affluence vs. Brand Switching Frequency')
plt.xlabel('Affluence Index')
plt.ylabel('Switching Frequency')
plt.tight_layout()
# Save figure
plt.savefig("Affluence vs Brand Switching Frequency.png", dpi=300, bbox_inches='tight')

plt.show()

# Plot 2: SEC vs Loyalty
plt.figure(figsize=(10, 6))
sns.boxplot(x='SEC', y='maxBrCd', data=df)
plt.title('Brand Loyalty by Socio-Economic Class (SEC)')
plt.xlabel('SEC')
plt.ylabel('Max Brand Share (Loyalty)')
plt.tight_layout()
# Save figure
plt.savefig("Brand Loyalty by Socio-Economic Class.png", dpi=300, bbox_inches='tight')

plt.show()

# Plot 3: Gender vs Promo Sensitivity (No Promo %)
plt.figure(figsize=(10, 6))
sns.boxplot(x='SEX', y='Pur Vol No Promo - %', data=df)
plt.title('Promo Avoidance by Gender')
plt.xlabel('Gender (1=Male, 2=Female)')
plt.ylabel('Volume Bought Without Promo (%)')
plt.tight_layout()
# Save figure
plt.savefig("Promo Avoidance by Gender.png", dpi=300, bbox_inches='tight')

plt.show()

# Plot 4: Age vs No. of Brands
plt.figure(figsize=(10, 6))
sns.boxplot(x='AGE', y='No. of Brands', data=df)
plt.title('Brand Variety by Age Group')
plt.xlabel('Age Group')
plt.ylabel('Number of Brands Purchased')
plt.tight_layout()
# Save figure
plt.savefig("Brand Variety by Age Group.png", dpi=300, bbox_inches='tight')

plt.show()












# Import necessary libraries

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

import warnings
warnings.filterwarnings("ignore", category=UserWarning, message="KMeans is known to have a memory leak*")

# Select features for clustering
cluster_features = [
    'No. of Brands', 'Brand Runs', 'Switching_Freq', 'maxBrCd',
    'Pur Vol No Promo - %', 'Pur Vol Promo 6 %', 'Pur Vol Other Promo %',
    'Total Volume', 'No. of  Trans', 'Value', 'Avg. Price'
]

# Drop missing values and scale features
df_cluster = df[cluster_features].dropna()
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_cluster)

# Elbow method to find optimal k
inertia = []
k_range = range(1, 11)
for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    inertia.append(kmeans.inertia_)

# Plot elbow curve
plt.figure(figsize=(8, 5))
plt.plot(k_range, inertia, marker='o')
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.xticks(k_range)
plt.grid(True)
plt.tight_layout()
# Save figure
plt.savefig("Elbow Method for Optimal k.png", dpi=300, bbox_inches='tight')

plt.show()



from sklearn.metrics import silhouette_score
import numpy as np

# Fit KMeans for k=3 and k=4
k_values = [3, 4]
kmeans_models = {}
labels_dict = {}
silhouette_scores = {}
cluster_profiles = {}

# Test K=3 and K=4
for k in [3, 4]:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X_scaled)
    df_temp = df_cluster.copy()
    df_temp['Cluster'] = labels
    profile = df_temp.groupby('Cluster').mean().round(2)
    cluster_profiles[k] = profile
    silhouette_scores[k] = silhouette_score(X_scaled, labels)

# Display profiles
print("\n--- Silhouette Scores ---")
print(silhouette_scores)

print("\n--- K=3 Cluster Profiles ---")
print(cluster_profiles[3])

print("\n--- K=4 Cluster Profiles ---")
print(cluster_profiles[4])



# Save profiles to Excel
with pd.ExcelWriter("Cluster_Profiles.xlsx") as writer:
    cluster_profiles[3].to_excel(writer, sheet_name="K=3")
    cluster_profiles[4].to_excel(writer, sheet_name="K=4")






from sklearn.decomposition import PCA

labels_dict = {}

for k in [3, 4]:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels_dict[k] = kmeans.fit_predict(X_scaled)
    
# Run PCA to reduce to 2D
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Create dataframes for plotting
df_pca_k3 = pd.DataFrame(X_pca, columns=["PC1", "PC2"])
df_pca_k3["Cluster"] = labels_dict[3]

df_pca_k4 = pd.DataFrame(X_pca, columns=["PC1", "PC2"])
df_pca_k4["Cluster"] = labels_dict[4]

# Plot PCA for K=3
plt.figure(figsize=(8, 6))
for cluster in df_pca_k3["Cluster"].unique():
    subset = df_pca_k3[df_pca_k3["Cluster"] == cluster]
    plt.scatter(subset["PC1"], subset["PC2"], label=f"Cluster {cluster}", alpha=0.6)
plt.title("PCA - KMeans Clusters (k=3)")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.legend()
plt.grid(True)
plt.tight_layout()
# Save figure
plt.savefig("PCA - KMeans Clusters-K-3.png", dpi=300, bbox_inches='tight')

plt.show()

# Plot PCA for K=4
plt.figure(figsize=(8, 6))
for cluster in df_pca_k4["Cluster"].unique():
    subset = df_pca_k4[df_pca_k4["Cluster"] == cluster]
    plt.scatter(subset["PC1"], subset["PC2"], label=f"Cluster {cluster}", alpha=0.6)
plt.title("PCA - KMeans Clusters (k=4)")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.legend()
plt.grid(True)
plt.tight_layout()
# Save figure
plt.savefig("PCA - KMeans Clusters-K-4.png", dpi=300, bbox_inches='tight')

plt.show()









# Define cluster names for K=3
cluster_names = {
    0: "Heavy Buyers / Variety Seekers",
    1: "Switchers / Promo Shoppers",
    2: "Loyalists"
}

# Recreate the k=3 clustering and apply labels
kmeans_k3 = KMeans(n_clusters=3, random_state=42, n_init=10)
df_cluster_k3 = df_cluster.copy()
df_cluster_k3["Cluster"] = kmeans_k3.fit_predict(X_scaled)
df_cluster_k3["Segment"] = df_cluster_k3["Cluster"].map(cluster_names)

# Preview the labeled dataset
df_cluster_k3.head()






# Save the labeled k=3 clustered dataset to an Excel file
output_path = "Clustered_Households_K3_Labeled.xlsx"
df_cluster_k3.to_excel(output_path, index=False)

output_path












from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder

# Merge cluster labels into the original dataset with household-level demographics
df_classification = df.copy()
df_classification = df_classification.loc[df_cluster_k3.index].copy()
df_classification["Segment"] = df_cluster_k3["Segment"]

# Select features for classification
features = [
    'SEC', 'Affluence Index', 'AGE', 'SEX', 'EDU',
    'Total Volume', 'No. of  Trans', 'Value', 'Avg. Price',
    'Pur Vol No Promo - %', 'Pur Vol Promo 6 %', 'Pur Vol Other Promo %'
]
target = 'Segment'

# Encode categorical columns and target
df_encoded = df_classification[features + [target]].dropna().copy()
le = LabelEncoder()
df_encoded[target] = le.fit_transform(df_encoded[target])  # Converts segment names to 0, 1, 2

# Split into train/test
X = df_encoded[features]
y = df_encoded[target]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale numeric features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

X_train.shape, X_test.shape









from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score

# Train Random Forest
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train_scaled, y_train)
rf_preds = rf.predict(X_test_scaled)

# Train Logistic Regression
lr = LogisticRegression(random_state=42, max_iter=1000)
lr.fit(X_train_scaled, y_train)
lr_preds = lr.predict(X_test_scaled)

# Evaluate both models
rf_report = classification_report(y_test, rf_preds, output_dict=True)
lr_report = classification_report(y_test, lr_preds, output_dict=True)

# Compare accuracy
rf_accuracy = accuracy_score(y_test, rf_preds)
lr_accuracy = accuracy_score(y_test, lr_preds)

# Prepare a summary comparison table
comparison_df = pd.DataFrame({
    "Model": ["Random Forest", "Logistic Regression"],
    "Accuracy": [rf_accuracy, lr_accuracy],
    "Loyalists F1": [rf_report['0']['f1-score'], lr_report['0']['f1-score']],
    "Switchers F1": [rf_report['1']['f1-score'], lr_report['1']['f1-score']],
    "Variety Seekers F1": [rf_report['2']['f1-score'], lr_report['2']['f1-score']],
})

comparison_df.round(3)






pip install xgboost


from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score
from xgboost import XGBClassifier

# Train models
rf = RandomForestClassifier(random_state=42).fit(X_train_scaled, y_train)
lr = LogisticRegression(max_iter=1000, random_state=42).fit(X_train_scaled, y_train)
xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42).fit(X_train_scaled, y_train)

# Predictions
rf_preds = rf.predict(X_test_scaled)
lr_preds = lr.predict(X_test_scaled)
xgb_preds = xgb.predict(X_test_scaled)

# Evaluate
models = {'Random Forest': rf_preds, 'Logistic Regression': lr_preds, 'XGBoost': xgb_preds}
for name, preds in models.items():
    print(f"\n {name}")
    print("Accuracy:", accuracy_score(y_test, preds))
    print(classification_report(y_test, preds, target_names=le.classes_))



# Generate feature importance for Random Forest and Logistic Regression (coefficients)
import numpy as np

# Feature names
feature_names = X.columns.tolist()

# Random Forest feature importance
rf_importance = rf.feature_importances_

# Logistic Regression absolute coefficient importance
lr_importance = np.abs(lr.coef_).mean(axis=0)

# Create DataFrame for plotting
importance_df = pd.DataFrame({
    "Feature": feature_names,
    "Random Forest": rf_importance,
    "Logistic Regression": lr_importance
}).set_index("Feature")

# Plot
importance_df.sort_values("Random Forest", ascending=True).plot.barh(figsize=(10, 6))
plt.title("Feature Importance Comparison (Random Forest vs Logistic Regression)")
plt.xlabel("Importance")
plt.tight_layout()
# Save figure
plt.savefig("Feature Importance Comparison.png", dpi=300, bbox_inches='tight')

plt.show()






import joblib

# Save models to files
joblib.dump(rf, "random_forest_segment_model.pkl")
joblib.dump(lr, "logistic_regression_segment_model.pkl")
joblib.dump(scaler, "scaler_for_models.pkl")
joblib.dump(le, "label_encoder_for_segments.pkl")

[
    "random_forest_segment_model.pkl",
    "logistic_regression_segment_model.pkl",
    "scaler_for_models.pkl",
    "label_encoder_for_segments.pkl"
]




